{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd0c8f1dbbf69973cc4b6583c327e15469e0625ce5e766b6de5a5209808bda28079",
   "display_name": "Python 3.7.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# MusicAI\n",
    "## Sztuczna Inteligencja - projekt (część 1 - sieć neuronowa)\n",
    "Autorzy: Jakub Ochnik, Adam Karabiniewicz, Marcel Bieniek\n",
    "___\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Part 1: data (audio) processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Theory"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Sound\n",
    "- essentially vibrations\n",
    "- represented by waveform (amplitude & time)\n",
    "### Waveform\n",
    "- T = period between two peaks (okres drgań)\n",
    "- frequency: f = 1/T, The lower the period, the higher the frequency\n",
    "- amplitude = max distance from y=0 to the peak\n",
    "- waveform function: y(t) = A*sin(2*pi*f*t + fi), fi = phase - it shifts the waveform to the right or to the left\n",
    "### Pitch and loudness\n",
    "- Higher frequency -> higher pitch (ton)\n",
    "- Longer periods -> Lower pitch, shorter periods -> higher pitch\n",
    "- Larger amplitude -> louder (głośność)\n",
    "### Digitalization\n",
    "- Acoustic sound waves (piano, voice) are continous, analog waveforms\n",
    "- We need them in a digital form\n",
    "- Analog digital conversion (ADC)\n",
    "- Sampling (sampling at a specific time intervals) -> Quantization (delimit samples by a number of bits)\n",
    "- We project the value of the amplitude at a sampling point to the closest discrete bit value\n",
    "- Sample rate (e.g.) 44100Hz, Bit depth (e.g.) 16 bits/channel\n",
    "### Fourier transform\n",
    "- How to work with real-world sound waves?\n",
    "- They are very complex and constantly changing :(\n",
    "- Resolution: Fourier transform\n",
    "- Decomposing complex periodic sound into sum of sine waves oscillating at different frequencies\n",
    "- Po polsku - rozbicie złożonego wykresu fali na wiele różnych, które są okresowe\n",
    "- The higher the amplitude, the more the wave contributes to the sound (dana składowa jest głośniejsza po prostu)\n",
    "### Practical usage of Fourier transform\n",
    "- Applying FFT on a sound returns a power spectrum (graph: Magnitude(frequency))\n",
    "- It shows which frequencies are dominating in a sound\n",
    "- Essentially transition from *time domain* to *frequency domain*\n",
    "- Problem: We lose time data\n",
    "- Power spectrum is a 'snapshot', a summary of all the frequencies happening during input sound's time window\n",
    "- Solution: Short Time Fourier transform\n",
    "### STFT\n",
    "- Computes several FFT at different intervals (fixed frame size, e.g. 2048 samples)\n",
    "- Preserves time information\n",
    "- Returns a spectrogram (time + frequency + magnitude)\n",
    "### Spectogram as a data source for deep learning\n",
    "- Spectrogram is an input for a deep learning model\n",
    "- In traditional ML preprocessing pipeline we work on a waveform and choose features we want to consider, etc.\n",
    "- With modern ML spectrogram model it's more straightforward\n",
    "### Mel Frequency Cepstral Coefficients (MFCCs)\n",
    "- Capture textural aspects of sound\n",
    "- Frequency domain features (time domain -> frequency domain)\n",
    "- They approximate human auditory system - that's their advantage over spectrograms\n",
    "- Calculated at each time frame\n",
    "- Result: coefficients vector (13 to 40), they are calculated at each frame\n",
    "- Finally: graph (time + MFCC coefficients + MFCCs)\n",
    "- Great for speech recognition, music genre classification, instrument classification, ...\n",
    "### MFCCs for deep learning\n",
    "- We pass MFCCs directly to a deep learning network\n",
    "- Very effective\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Preprocessing audio"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Importing necessary libraries and packages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import librosa, librosa.display # api for visualizing data\n",
    "import matplotlib.pyplot as plt"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "Load audio file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}