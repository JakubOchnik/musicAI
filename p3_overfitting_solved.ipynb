{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd0c8f1dbbf69973cc4b6583c327e15469e0625ce5e766b6de5a5209808bda28079",
   "display_name": "Python 3.7.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# MusicAI\n",
    "## Sztuczna Inteligencja - projekt \n",
    "### Część 3 - prosta sieć neuronowa\n",
    "Autorzy: Jakub Ochnik, Adam Karabiniewicz, Marcel Bieniek\n",
    "___\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Importing necessary libraries and packages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "source": [
    "Constants"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"Data\\\\data_full.json\""
   ]
  },
  {
   "source": [
    "Loading dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    \"\"\"Loads training dataset from json file.\n",
    "        :param data_path (str): Path to json file containing data\n",
    "        :return X (ndarray): Inputs\n",
    "        :return y (ndarray): Targets\n",
    "    \"\"\"\n",
    "\n",
    "    with open(data_path, \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "\n",
    "    # convert lists to numpy arrays\n",
    "    \"\"\"\n",
    "    inputs -> X\n",
    "    outputs -> y\n",
    "    \"\"\"\n",
    "\n",
    "    X = np.array(data[\"mfcc\"])\n",
    "    y = np.array(data[\"labels\"])\n",
    "    names = data[\"mapping\"]\n",
    "\n",
    "    print(\"Data succesfully loaded!\")\n",
    "\n",
    "    return  X, y, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, names = load_data(DATA_PATH)\n",
    "\n",
    "print(len(X))\n",
    "print(len(y))"
   ]
  },
  {
   "source": [
    "Splitting the data into train and test sets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "source": [
    "Build the network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer\n",
    "# 3 hidden layers\n",
    "# simple multilayered perceptron (5)\n",
    "# fully connected (dense) layers\n",
    "model = keras.Sequential([\n",
    "    # input layer\n",
    "    # flatten takes multidim array and flattens it out\n",
    "    # json contains 3D array, index 0 represents different segments (different segments), 1 & 2 represent actual data shape\n",
    "    keras.layers.Flatten(input_shape=(X.shape[1],X.shape[2])),\n",
    "    # MOZNA FAJNIE OPISAC RELU W SPRAWKU\n",
    "    # ReLU is better for training than sigmoid, better convergence, reduced likelihood of vanishing gradient\n",
    "    # 1st hidden layer\n",
    "    keras.layers.Dense(512, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    # 2nd hidden layer\n",
    "    keras.layers.Dense(256, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    # 3rd hidden layer\n",
    "    keras.layers.Dense(64, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    # output layer\n",
    "    # 10 neurons = 10 genres\n",
    "    # softmax\n",
    "    keras.layers.Dense(10, activation=\"softmax\")    \n",
    "])"
   ]
  },
  {
   "source": [
    "Compiling the network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam - extension of classic gradient descent, effective\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "source": [
    "Training the network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batching - the way of how we train network\n",
    "# stochastic gradient descent (quick, inaccurate), full batch (compute gradient on the whole training set, slow, accurate), mini-batch (compute gradient on a subset of dataset, 16-128 samples, the best of 2 worlds)\n",
    "# batch_size - number of samples of mini-batch\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32)"
   ]
  },
  {
   "source": [
    "Plot accuracy and error over epochs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    fig, axs = plt.subplots(2)\n",
    "    # accuracy subplot\n",
    "    axs[0].plot(history.history[\"accuracy\"], label=\"train accuracy\")\n",
    "    axs[0].plot(history.history[\"val_accuracy\"], label=\"test accuracy\")\n",
    "\n",
    "    axs[0].set_ylabel(\"Accuracy\")\n",
    "    axs[0].legend(loc=\"lower right\")\n",
    "    axs[0].set_title(\"Accuracy eval\")\n",
    "\n",
    "    # error subplot\n",
    "    axs[1].plot(history.history[\"loss\"], label=\"train error\")\n",
    "    axs[1].plot(history.history[\"val_loss\"], label=\"test error\")\n",
    "\n",
    "    axs[1].set_ylabel(\"Error\")\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].legend(loc=\"upper right\")\n",
    "    axs[1].set_title(\"Error eval\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error, test_accuracy = model.evaluate(X_test, y_test, verbose = 1)\n",
    "print(\"Model accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "source": [
    "Testing the network on real-world examples"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sample(model, X):\n",
    "    # X -> 2D array (130, 13)\n",
    "    X = X[np.newaxis, ...]\n",
    "    predictions = model.predict(X)\n",
    "    predicted_index = np.argmax(predictions, axis=1)\n",
    "    #print(predicted_index)\n",
    "    return predicted_index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# testing a specific sample\n",
    "x_n = X_test[103]\n",
    "y_n = y_test[103]\n",
    "\n",
    "predict_sample(model, x_n)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_song(model, data, orig_genre, names):\n",
    "    # X -> 2D array (130, 13)\n",
    "    data = np.array(data)\n",
    "    preds = []\n",
    "    for X in data:\n",
    "        preds.append(predict_sample(model, X))\n",
    "    predicted = np.bincount(preds).argmax()\n",
    "    print(\"Expected genre: {}, Predicted genre: {}\".format(orig_genre,names[predicted]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import math\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_song(filename, n_mfcc=13, n_fft=2048, hop_length=512, num_segments=10):\n",
    "    #file_path = os.path.join(dirpath, f)\n",
    "    SAMPLE_RATE = 22050\n",
    "    signal, sr = librosa.load(filename, sr = 22050)\n",
    "    DURATION = 30 # seconds\n",
    "    SAMPLES_PER_TRACK = SAMPLE_RATE * DURATION\n",
    "    num_samples_per_segment = SAMPLES_PER_TRACK // num_segments\n",
    "    expected_num_mfcc_vectors_per_segment = math.ceil(num_samples_per_segment / hop_length) # todo         \n",
    "    data = []            \n",
    "    # divide into segments\n",
    "    for s in range(num_segments):\n",
    "        start_sample = num_samples_per_segment * s\n",
    "        finish_sample = start_sample + num_samples_per_segment\n",
    "\n",
    "        mfcc = librosa.feature.mfcc(signal[start_sample:finish_sample], sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "        mfcc = mfcc.T\n",
    "        \n",
    "        # store mfcc for segment if it has the expected length\n",
    "        if len(mfcc) == expected_num_mfcc_vectors_per_segment:\n",
    "            data.append(mfcc.tolist()) #conv. from np array to list\n",
    "            #print(\"{}, segment: {}\".format(filename, s))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_to_pred = load_song(\"Data/jazz_piano.wav\")\n",
    "predict_song(model, x_to_pred, \"jazz\", names)\n",
    "\n",
    "x_to_pred = load_song(\"Data/pop_rock.wav\")\n",
    "predict_song(model, x_to_pred, \"pop/rock\", names)\n",
    "\n",
    "x_to_pred = load_song(\"Data/classic_piano.wav\")\n",
    "predict_song(model, x_to_pred, \"classical\", names)\n",
    "\n",
    "x_to_pred = load_song(\"Data/classic_symphony.wav\")\n",
    "predict_song(model, x_to_pred, \"classical\", names)\n",
    "\n",
    "x_to_pred = load_song(\"Data/blues.wav\")\n",
    "predict_song(model, x_to_pred, \"blues\", names)\n",
    "\n",
    "x_to_pred = load_song(\"Data/blues_2.wav\")\n",
    "predict_song(model, x_to_pred, \"blues\", names)\n",
    "\n",
    "x_to_pred = load_song(\"Data/classic_piano2.wav\")\n",
    "predict_song(model, x_to_pred, \"classical\", names)\n",
    "\n",
    "x_to_pred = load_song(\"Data/mozart.wav\")\n",
    "predict_song(model, x_to_pred, \"classical\", names)\n",
    "\n",
    "x_to_pred = load_song(\"Data/rock_metal.wav\")\n",
    "predict_song(model, x_to_pred, \"rock/metal\", names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}